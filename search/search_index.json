{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RSL Euler Cluster Guide","text":""},{"location":"#quick-access-to-all-sections","title":"\ud83d\ude80 Quick Access to All Sections","text":""},{"location":"#1-getting-started","title":"1. Getting Started","text":"<p>Access Requirements, SSH Setup, Verification \u2192 - Getting cluster access - Setting up SSH connection - Verifying RSL group membership</p>"},{"location":"#2-data-management","title":"2. Data Management","text":"<p>Storage Locations and Quotas \u2192 - Home, Scratch, Project, Work directories - Storage quotas and best practices - Using local scratch ($TMPDIR)</p>"},{"location":"#3-python-environments-ml-training","title":"3. Python Environments &amp; ML Training","text":"<p>Miniconda Setup and Training Workflows \u2192 - Installing and managing Miniconda - Creating conda environments - Complete ML training workflow</p>"},{"location":"#4-computing-on-euler","title":"4. Computing on Euler","text":"<p>Interactive Sessions and Batch Jobs \u2192 - Requesting interactive sessions - Writing and submitting SLURM job scripts - GPU selection and multi-GPU training</p>"},{"location":"#5-container-workflow","title":"5. Container Workflow","text":"<p>Docker/Singularity Guide \u2192 - Building Docker containers - Converting to Singularity - Running containerized jobs</p>"},{"location":"#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Complete Reference Guide - All sections in one document</li> <li>Scripts Library - Ready-to-use job scripts</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"#quick-start","title":"\ud83c\udfaf Quick Start","text":"<pre><code># SSH to Euler\nssh &lt;nethz_username&gt;@euler.ethz.ch\n\n# Check RSL membership\nmy_share_info\n\n# Submit a job\nsbatch job.sh\n</code></pre> <p>Maintained by the Robotics Systems Lab (RSL), ETH Zurich</p>"},{"location":"complete-guide/","title":"Complete RSL Euler Cluster Guide","text":"<p>This guide provides comprehensive documentation for using the Euler HPC cluster at ETH Zurich as a member of the Robotics Systems Lab (RSL).</p>"},{"location":"complete-guide/#guide-sections","title":"\ud83d\udcda Guide Sections","text":""},{"location":"complete-guide/#1-getting-started","title":"1. Getting Started","text":"<ul> <li>Access requirements and approval process</li> <li>SSH configuration and key setup</li> <li>Verifying RSL group membership</li> <li>Initial setup script</li> </ul>"},{"location":"complete-guide/#2-data-management","title":"2. Data Management","text":"<ul> <li>Understanding storage locations (home, scratch, project, work)</li> <li>Storage quotas and limits</li> <li>Best practices for data organization</li> <li>Using local scratch ($TMPDIR) effectively</li> </ul>"},{"location":"complete-guide/#3-python-environments-ml-training","title":"3. Python Environments &amp; ML Training","text":"<ul> <li>Installing Miniconda in the correct location</li> <li>Creating and managing conda environments</li> <li>Complete ML training workflow example</li> <li>Performance optimization tips</li> </ul>"},{"location":"complete-guide/#4-computing-on-euler","title":"4. Computing on Euler","text":"<ul> <li>Interactive sessions for development</li> <li>Writing SLURM batch job scripts</li> <li>GPU selection and memory management</li> <li>Job monitoring and debugging</li> </ul>"},{"location":"complete-guide/#5-container-workflow","title":"5. Container Workflow","text":"<ul> <li>Building Docker containers locally</li> <li>Converting to Singularity format</li> <li>Deploying and running on Euler</li> <li>Performance considerations</li> </ul>"},{"location":"complete-guide/#additional-resources","title":"\ud83d\udd27 Additional Resources","text":"<ul> <li>Scripts Library - Ready-to-use SLURM scripts and examples</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"complete-guide/#quick-start-checklist","title":"\ud83d\ude80 Quick Start Checklist","text":"<ol> <li>\u2705 Get access approval (fill out form or contact Manthan Patel)</li> <li>\u2705 Set up SSH keys and config</li> <li>\u2705 Run the setup script to verify access and create directories</li> <li>\u2705 Install Miniconda in <code>/cluster/project/rsl/$USER/</code></li> <li>\u2705 Test with an interactive session</li> <li>\u2705 Submit your first batch job</li> </ol>"},{"location":"complete-guide/#support","title":"\ud83d\udcde Support","text":"<ul> <li>RSL-specific issues: Contact your supervisor or Manthan Patel</li> <li>General Euler support: ETH IT ServiceDesk</li> <li>Guide improvements: GitHub Issues</li> </ul> <p>This guide is maintained by the Robotics Systems Lab (RSL) at ETH Zurich</p>"},{"location":"computing-guide/","title":"Computing on Euler","text":"<p>This guide covers interactive sessions and batch job submission on the Euler cluster.</p>"},{"location":"computing-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Interactive Sessions</li> <li>Batch Jobs with SLURM</li> <li>Job Monitoring and Management</li> <li>Best Practices</li> </ol>"},{"location":"computing-guide/#interactive-sessions","title":"\ud83d\udda5\ufe0f Interactive Sessions","text":"<p>Interactive sessions on Euler allow you to work directly on compute nodes with allocated resources. This is essential for development, debugging, and testing before submitting batch jobs.</p>"},{"location":"computing-guide/#requesting-interactive-sessions","title":"\ud83d\ude80 Requesting Interactive Sessions","text":"<p>The basic command to request an interactive session is:</p> <pre><code>srun --pty bash\n</code></pre> <p>This gives you a basic session with default resources. For more control, specify your requirements:</p>"},{"location":"computing-guide/#common-interactive-session-configurations","title":"\ud83d\udcca Common Interactive Session Configurations","text":""},{"location":"computing-guide/#basic-cpu-session","title":"Basic CPU Session","text":"<pre><code># 2 hours, 8 CPUs, 32GB RAM\nsrun --time=2:00:00 --cpus-per-task=8 --mem=32G --pty bash\n</code></pre>"},{"location":"computing-guide/#gpu-development-session","title":"GPU Development Session","text":"<pre><code># 4 hours, 1 GPU, 16 CPUs, 64GB RAM, 100GB local scratch\nsrun --time=4:00:00 --gpus=1 --cpus-per-task=16 --mem=64G --tmp=100G --pty bash\n</code></pre>"},{"location":"computing-guide/#multi-gpu-session","title":"Multi-GPU Session","text":"<pre><code># 2 hours, 4 GPUs, 32 CPUs, 128GB RAM\nsrun --time=2:00:00 --gpus=4 --cpus-per-task=32 --mem=128G --pty bash\n</code></pre>"},{"location":"computing-guide/#high-memory-session","title":"High Memory Session","text":"<pre><code># 1 hour, 4 CPUs, 256GB RAM\nsrun --time=1:00:00 --cpus-per-task=4 --mem=256G --pty bash\n</code></pre>"},{"location":"computing-guide/#working-in-interactive-sessions","title":"\ud83d\udd27 Working in Interactive Sessions","text":"<p>Once your session starts, you'll be on a compute node:</p> <pre><code># Check your allocated resources\necho \"Hostname: $(hostname)\"\necho \"CPUs: $(nproc)\"\necho \"Memory: $(free -h | grep Mem | awk '{print $2}')\"\necho \"GPUs: $CUDA_VISIBLE_DEVICES\"\necho \"Local scratch: $TMPDIR\"\n\n# Load necessary modules\nmodule load eth_proxy\n\n# Activate your conda environment\nconda activate myenv\n\n# For GPU sessions, verify CUDA\nnvidia-smi\n</code></pre>"},{"location":"computing-guide/#interactive-development-workflow","title":"\ud83d\udcdd Interactive Development Workflow","text":""},{"location":"computing-guide/#1-code-development-with-gpu","title":"1. Code Development with GPU","text":"<pre><code># Request GPU session\nsrun --gpus=1 --mem=32G --time=2:00:00 --pty bash\n\n# Navigate to your code\ncd /cluster/home/$USER/my_project\n\n# Run and debug\npython train.py --debug\n</code></pre>"},{"location":"computing-guide/#2-interactive-pythonipython","title":"2. Interactive Python/IPython","text":"<pre><code># In your interactive session\nmodule load eth_proxy\nconda activate myenv\n\n# Start IPython with GPU support\nipython\n\n# In IPython:\n# &gt;&gt;&gt; import torch\n# &gt;&gt;&gt; torch.cuda.is_available()\n# &gt;&gt;&gt; # Interactive debugging here\n</code></pre>"},{"location":"computing-guide/#3-container-development","title":"3. Container Development","text":"<pre><code># Request session with local scratch\nsrun --gpus=1 --tmp=50G --mem=32G --pty bash\n\n# Extract container to local scratch\ntar -xf /cluster/work/rsl/$USER/containers/dev.tar -C $TMPDIR\n\n# Enter container interactively\nsingularity shell --nv \\\n    --bind /cluster/project/rsl/$USER:/project \\\n    $TMPDIR/dev.sif\n\n# Now you're inside the container for testing\n</code></pre>"},{"location":"computing-guide/#interactive-jupyter-sessions","title":"\ud83c\udf10 Interactive Jupyter Sessions","text":"<p>ETH provides JupyterHub access to Euler:</p> <ol> <li>Access via browser: https://jupyter.euler.hpc.ethz.ch</li> <li>Login with your nethz credentials</li> <li>Select resources (GPUs, memory, time)</li> <li>Your notebook runs on Euler compute nodes</li> </ol>"},{"location":"computing-guide/#launching-jupyter-from-command-line","title":"Launching Jupyter from Command Line","text":"<pre><code># In an interactive session\nsrun --gpus=1 --mem=32G --time=4:00:00 --pty bash\n\n# Load modules\nmodule load eth_proxy\n\n# Start Jupyter (note the token in output)\njupyter notebook --no-browser --ip=$(hostname -i)\n\n# From your local machine, create SSH tunnel:\n# ssh -L 8888:compute-node:8888 euler\n# Then open http://localhost:8888 in your browser\n</code></pre>"},{"location":"computing-guide/#vscode-remote-development","title":"\ud83d\udcbb VSCode Remote Development","text":"<p>You can use VSCode directly on Euler nodes:</p>"},{"location":"computing-guide/#option-1-via-jupyterhub","title":"Option 1: Via JupyterHub","text":"<ol> <li>Go to https://jupyter.euler.hpc.ethz.ch</li> <li>Select \"Code Server\" instead of JupyterLab</li> <li>VSCode opens in your browser with Euler resources</li> </ol>"},{"location":"computing-guide/#option-2-ssh-remote-development","title":"Option 2: SSH Remote Development","text":"<pre><code># First, request an interactive session\nsrun --gpus=1 --mem=32G --time=4:00:00 --pty bash\n\n# Note the compute node name (e.g., eu-g1-001)\nhostname\n\n# From your local VSCode:\n# 1. Install \"Remote - SSH\" extension\n# 2. Connect to: ssh username@euler\n# 3. Then SSH to the compute node from terminal\n</code></pre>"},{"location":"computing-guide/#time-limits-and-best-practices","title":"\u23f0 Time Limits and Best Practices","text":""},{"location":"computing-guide/#time-limits","title":"Time Limits","text":"<ul> <li>Default: 1 hour if not specified</li> <li>Maximum interactive time: 24 hours</li> <li>GPU sessions may have shorter limits during peak usage</li> </ul>"},{"location":"computing-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Request only what you need - Others are waiting for resources</li> <li>Use <code>--tmp</code> for I/O intensive work - Local scratch is much faster</li> <li>Exit when done - Don't leave idle sessions</li> <li>Save your work frequently - Sessions can be terminated</li> <li>Use screen/tmux for long sessions - Protects against disconnections</li> </ol>"},{"location":"computing-guide/#common-issues-and-solutions","title":"\u2757 Common Issues and Solutions","text":"<p>Session won't start (pending) <pre><code># Check queue status\nsqueue -u $USER\n\n# Check available resources\nsinfo -p gpu\n\n# Try requesting fewer resources or different partition\n</code></pre></p> <p>Disconnection from interactive session <pre><code># Prevent with screen/tmux\nscreen -S mysession\nsrun --gpus=1 --pty bash\n\n# Detach: Ctrl+A, D\n# Reattach: screen -r mysession\n</code></pre></p> <p>Out of memory in session <pre><code># Monitor memory usage\nwatch -n 1 free -h\n\n# Check your process memory\nps aux | grep $USER\n\n# Request more memory next time\n</code></pre></p>"},{"location":"computing-guide/#quick-reference-card","title":"\ud83d\udccb Quick Reference Card","text":"Task Command Basic session <code>srun --pty bash</code> GPU session <code>srun --gpus=1 --pty bash</code> Specific time <code>srun --time=4:00:00 --pty bash</code> More memory <code>srun --mem=64G --pty bash</code> Local scratch <code>srun --tmp=100G --pty bash</code> Check allocation <code>scontrol show job $SLURM_JOB_ID</code> Exit session <code>exit</code> or <code>Ctrl+D</code>"},{"location":"computing-guide/#batch-jobs-with-slurm","title":"\ud83d\udcdd Batch Jobs with SLURM","text":"<p>SLURM batch scripts allow you to submit jobs that run without manual intervention. Here are tested examples for common use cases on the Euler cluster.</p>"},{"location":"computing-guide/#basic-job-script-template","title":"\ud83c\udfaf Basic Job Script Template","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=my_job\n#SBATCH --output=logs/job_%j.out\n#SBATCH --error=logs/job_%j.err\n#SBATCH --time=04:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=4G\n\n# Load required modules\nmodule load eth_proxy\n\n# Job info\necho \"Job started on $(hostname) at $(date)\"\necho \"Job ID: $SLURM_JOB_ID\"\n\n# Your commands here\ncd /cluster/home/$USER/my_project\npython my_script.py\n\necho \"Job completed at $(date)\"\n</code></pre> <p>Submit with: <code>sbatch my_job.sh</code></p>"},{"location":"computing-guide/#gpu-selection-and-memory-requirements","title":"\ud83c\udfae GPU Selection and Memory Requirements","text":""},{"location":"computing-guide/#requesting-specific-gpu-types","title":"Requesting Specific GPU Types","text":"<pre><code># Request any available GPU\n#SBATCH --gpus=1\n\n# Request specific GPU model\n#SBATCH --gpus=nvidia_geforce_rtx_4090:1     # RTX 4090 (24GB VRAM)\n#SBATCH --gpus=nvidia_geforce_rtx_3090:1     # RTX 3090 (24GB VRAM)\n#SBATCH --gpus=nvidia_a100_80gb_pcie:1       # A100 (80GB VRAM)\n#SBATCH --gpus=nvidia_a100-pcie-40gb:1       # A100 (40GB VRAM)\n#SBATCH --gpus=tesla_v100-sxm2-32gb:1        # V100 (32GB VRAM)\n\n# Request multiple GPUs of same type\n#SBATCH --gpus=nvidia_geforce_rtx_4090:4     # 4x RTX 4090\n</code></pre>"},{"location":"computing-guide/#gpu-memory-vs-system-memory","title":"GPU Memory vs System Memory","text":"<pre><code># System memory (RAM) - shared by CPUs\n#SBATCH --mem=64G              # Total memory for job\n#SBATCH --mem-per-cpu=8G       # Memory per CPU core\n\n# GPU memory (VRAM) is fixed by GPU type:\n# RTX 4090: 24GB VRAM\n# RTX 3090: 24GB VRAM  \n# A100: 40GB or 80GB VRAM\n# V100: 32GB VRAM\n# RTX 2080 Ti: 11GB VRAM\n</code></pre>"},{"location":"computing-guide/#example-large-model-training","title":"Example: Large Model Training","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=llm-training\n#SBATCH --gpus=nvidia_a100_80gb_pcie:1  # Need 80GB VRAM for model\n#SBATCH --cpus-per-task=32               # Many CPUs for data loading\n#SBATCH --mem=256G                       # Large system RAM for dataset\n#SBATCH --time=72:00:00\n#SBATCH --tmp=500G                       # Local scratch for dataset\n\nmodule load eth_proxy\n\n# The A100 80GB GPU allows loading larger models\n# System RAM (256GB) is for CPU operations and data loading\n# GPU VRAM (80GB) is for model weights and activations\n</code></pre>"},{"location":"computing-guide/#gpu-training-script","title":"\ud83d\ude80 GPU Training Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=gpu-training\n#SBATCH --output=logs/train_%j.out\n#SBATCH --error=logs/train_%j.err\n#SBATCH --time=24:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=16\n#SBATCH --mem-per-cpu=4G\n#SBATCH --gpus=1                # Request any available GPU\n#SBATCH --tmp=100G\n\n# For specific GPU types, use one of these instead:\n# #SBATCH --gpus=nvidia_geforce_rtx_4090:1     # RTX 4090 (24GB)\n# #SBATCH --gpus=nvidia_geforce_rtx_3090:1     # RTX 3090 (24GB)  \n# #SBATCH --gpus=nvidia_a100_80gb_pcie:1       # A100 80GB\n# #SBATCH --gpus=nvidia_a100-pcie-40gb:1       # A100 40GB\n# #SBATCH --gpus=tesla_v100-sxm2-32gb:1        # V100 32GB\n\n# Load modules\nmodule load eth_proxy\n\n# Job information\necho \"=========================================\"\necho \"SLURM Job ID: $SLURM_JOB_ID\"\necho \"Running on: $(hostname)\"\necho \"Starting at: $(date)\"\necho \"GPU allocation: $CUDA_VISIBLE_DEVICES\"\necho \"=========================================\"\n\n# Copy dataset to local scratch for faster I/O\necho \"Copying dataset to local scratch...\"\ncp -r /cluster/scratch/$USER/datasets/my_dataset $TMPDIR/\n\n# Activate conda environment\nsource /cluster/project/rsl/$USER/miniconda3/bin/activate\nconda activate ml_env\n\n# Run training\ncd /cluster/home/$USER/my_ml_project\npython train.py \\\n    --data-dir $TMPDIR/my_dataset \\\n    --output-dir /cluster/project/rsl/$USER/results/$SLURM_JOB_ID \\\n    --epochs 100 \\\n    --batch-size 64 \\\n    --lr 0.001\n\n# Copy final results back\necho \"Copying results...\"\ncp -r $TMPDIR/checkpoints/* /cluster/project/rsl/$USER/checkpoints/\n\necho \"Job completed at $(date)\"\n</code></pre>"},{"location":"computing-guide/#multi-gpu-distributed-training","title":"\ud83d\udd25 Multi-GPU Distributed Training","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=distributed-training\n#SBATCH --output=logs/distributed_%j.out\n#SBATCH --error=logs/distributed_%j.err\n#SBATCH --time=48:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=32\n#SBATCH --mem-per-cpu=8G\n#SBATCH --gpus=4\n#SBATCH --tmp=200G\n\nmodule load eth_proxy\n\necho \"Multi-GPU training on $(hostname)\"\necho \"GPUs: $CUDA_VISIBLE_DEVICES\"\necho \"Number of GPUs: $(echo $CUDA_VISIBLE_DEVICES | tr ',' '\\n' | wc -l)\"\n\n# Prepare data on local scratch\ntar -xf /cluster/scratch/$USER/datasets/imagenet.tar -C $TMPDIR/\n\n# Activate environment\nsource /cluster/project/rsl/$USER/miniconda3/bin/activate\nconda activate pytorch_env\n\n# Set distributed training environment variables\nexport MASTER_ADDR=$(hostname)\nexport MASTER_PORT=29500\nexport WORLD_SIZE=4\n\n# Run distributed training\ncd /cluster/home/$USER/vision_project\npython -m torch.distributed.run \\\n    --nproc_per_node=4 \\\n    --master_addr=$MASTER_ADDR \\\n    --master_port=$MASTER_PORT \\\n    train_distributed.py \\\n    --data $TMPDIR/imagenet \\\n    --output /cluster/project/rsl/$USER/results/$SLURM_JOB_ID \\\n    --sync-bn \\\n    --amp\n\necho \"Training completed at $(date)\"\n</code></pre>"},{"location":"computing-guide/#array-jobs-for-parallel-processing","title":"\ud83d\udd04 Array Jobs for Parallel Processing","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=param-sweep\n#SBATCH --output=logs/array_%A_%a.out\n#SBATCH --error=logs/array_%A_%a.err\n#SBATCH --time=02:00:00\n#SBATCH --array=1-50\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=4G\n#SBATCH --gpus=1\n\nmodule load eth_proxy\n\n# Array job information\necho \"Array Job ID: $SLURM_ARRAY_JOB_ID\"\necho \"Array Task ID: $SLURM_ARRAY_TASK_ID\"\necho \"Running on: $(hostname)\"\n\n# Define parameter arrays\nlearning_rates=(0.001 0.0001 0.00001 0.01 0.1)\nbatch_sizes=(16 32 64 128 256)\n\n# Calculate indices for 2D parameter grid\nlr_index=$(( ($SLURM_ARRAY_TASK_ID - 1) / ${#batch_sizes[@]} ))\nbs_index=$(( ($SLURM_ARRAY_TASK_ID - 1) % ${#batch_sizes[@]} ))\n\nLR=${learning_rates[$lr_index]}\nBS=${batch_sizes[$bs_index]}\n\necho \"Testing LR=$LR, Batch Size=$BS\"\n\n# Activate environment\nsource /cluster/project/rsl/$USER/miniconda3/bin/activate\nconda activate ml_env\n\n# Run experiment\ncd /cluster/home/$USER/hyperparameter_search\npython train.py \\\n    --lr $LR \\\n    --batch-size $BS \\\n    --epochs 20 \\\n    --output /cluster/project/rsl/$USER/hp_search/lr${LR}_bs${BS} \\\n    --seed $SLURM_ARRAY_TASK_ID\n</code></pre> <p>Submit array job: <code>sbatch --array=1-25 array_job.sh</code></p>"},{"location":"computing-guide/#container-based-job","title":"\ud83d\udce6 Container-Based Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=container-job\n#SBATCH --output=logs/container_%j.out\n#SBATCH --error=logs/container_%j.err\n#SBATCH --time=12:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=8G\n#SBATCH --gpus=2\n#SBATCH --tmp=150G\n\nmodule load eth_proxy\n\necho \"Container job started on $(hostname)\"\necho \"Extracting container to local scratch...\"\n\n# Extract container (much faster than running from /cluster/work)\ntime tar -xf /cluster/work/rsl/$USER/containers/ml_stack.tar -C $TMPDIR\n\n# Prepare data\necho \"Preparing data...\"\nmkdir -p $TMPDIR/data\ncp -r /cluster/scratch/$USER/datasets/train_data $TMPDIR/data/\n\n# Run training in container\necho \"Starting training...\"\nsingularity exec \\\n    --nv \\\n    --bind $TMPDIR/data:/data:ro \\\n    --bind /cluster/project/rsl/$USER/results/$SLURM_JOB_ID:/output \\\n    --bind /cluster/project/rsl/$USER/checkpoints:/checkpoints \\\n    $TMPDIR/ml_stack.sif \\\n    python /app/train.py \\\n        --data /data/train_data \\\n        --output /output \\\n        --checkpoint-dir /checkpoints \\\n        --resume-from latest\n\necho \"Job completed at $(date)\"\n</code></pre>"},{"location":"computing-guide/#job-monitoring-and-management","title":"\ud83d\udd0d Job Monitoring and Management","text":""},{"location":"computing-guide/#useful-slurm-commands","title":"Useful SLURM Commands","text":"<pre><code># Submit job\nsbatch my_job.sh\n\n# Check job status\nsqueue -u $USER\n\n# Detailed job info\nscontrol show job &lt;job_id&gt;\n\n# Cancel job\nscancel &lt;job_id&gt;\n\n# Cancel all your jobs\nscancel -u $USER\n\n# View job efficiency after completion\nseff &lt;job_id&gt;\n\n# Monitor job in real-time\nwatch -n 10 squeue -u $USER\n</code></pre>"},{"location":"computing-guide/#debugging-failed-jobs","title":"\ud83d\udee0\ufe0f Debugging Failed Jobs","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=debug-job\n#SBATCH --output=logs/debug_%j.out\n#SBATCH --error=logs/debug_%j.err\n#SBATCH --time=00:30:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=4G\n#SBATCH --gpus=1\n\n# Enable bash debugging\nset -e  # Exit on error\nset -u  # Exit on undefined variable\nset -x  # Print commands as they execute\n\nmodule load eth_proxy\n\n# Print environment for debugging\necho \"=== Environment ===\"\nenv | grep SLURM\necho \"=== GPU Info ===\"\nnvidia-smi\necho \"=== Memory Info ===\"\nfree -h\necho \"=== Disk Space ===\"\ndf -h $TMPDIR\n\n# Your actual commands with error checking\nif ! python --version; then\n    echo \"Python not found!\"\n    exit 1\nfi\n\n# Run with explicit error handling\npython my_script.py || {\n    echo \"Script failed with exit code $?\"\n    echo \"Current directory: $(pwd)\"\n    echo \"Files present: $(ls -la)\"\n    exit 1\n}\n</code></pre>"},{"location":"computing-guide/#best-practices_1","title":"\ud83d\udca1 Best Practices","text":""},{"location":"computing-guide/#best-practices-for-job-scripts","title":"Best Practices for Job Scripts","text":"<ol> <li>Always specify time limits - Jobs without time limits may be deprioritized</li> <li>Create log directories - <code>mkdir -p logs</code> before submitting</li> <li>Use local scratch ($TMPDIR) - Much faster than network storage</li> <li>Request appropriate resources - Don't over-request, it delays your job</li> <li>Use job arrays - For embarrassingly parallel tasks</li> <li>Add error handling - Check exit codes and add recovery logic</li> </ol>"},{"location":"computing-guide/#job-script-checklist","title":"\ud83d\udcdd Job Script Checklist","text":"<p>Before submitting your job, verify:</p> <ul> <li> Shebang line: <code>#!/bin/bash</code></li> <li> Job name is descriptive</li> <li> Output/error paths exist (<code>mkdir -p logs</code>)</li> <li> Time limit is appropriate</li> <li> Memory request is reasonable</li> <li> GPU request matches your code</li> <li> Module <code>eth_proxy</code> is loaded</li> <li> Paths use <code>$USER</code> variable</li> <li> Local scratch <code>$TMPDIR</code> used for I/O</li> <li> Results saved to persistent storage</li> </ul>"},{"location":"computing-guide/#test-scripts","title":"\ud83e\uddea Test Scripts","text":"<p>We provide test scripts for all computing scenarios:</p>"},{"location":"computing-guide/#interactive-sessions_1","title":"Interactive Sessions","text":"<ul> <li>Test scripts are provided inline in the examples above</li> </ul>"},{"location":"computing-guide/#batch-jobs","title":"Batch Jobs","text":"<ul> <li>test_cpu_job.sh - Basic CPU job submission</li> <li>test_basic_cpu.sh - Alternative CPU test</li> <li>test_gpu_job.sh - GPU allocation test</li> <li>test_gpu_specific.sh - Specific GPU type selection (RTX 4090)</li> <li>test_array_job.sh - Array job for parameter sweeps</li> </ul> <p>To run the tests: <pre><code># Test basic job submission\nsbatch test_cpu_job.sh\n\n# Test GPU allocation\nsbatch test_gpu_job.sh\n\n# Test specific GPU request\nsbatch test_gpu_specific.sh\n\n# Test array jobs (creates 6 tasks)\nsbatch test_array_job.sh\n</code></pre></p>"},{"location":"container-workflow/","title":"Container Workflow","text":"<p>This page provides a detailed, tested workflow for deploying containerized applications on the Euler cluster.</p>"},{"location":"container-workflow/#overview","title":"Overview","text":"<p>The workflow consists of four main steps: 1. Build a Docker container locally 2. Convert to Singularity format 3. Transfer to the cluster 4. Run using SLURM</p>"},{"location":"container-workflow/#step-1-building-docker-containers","title":"Step 1: Building Docker Containers","text":""},{"location":"container-workflow/#basic-dockerfile-template","title":"Basic Dockerfile Template","text":"<pre><code>FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3 \\\n    python3-pip \\\n    git \\\n    wget \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python packages\nRUN pip3 install --no-cache-dir \\\n    torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118 \\\n    numpy \\\n    scipy \\\n    matplotlib\n\n# Set working directory\nWORKDIR /workspace\n\n# Copy application code\nCOPY . .\n\n# Set entrypoint\nENTRYPOINT [\"python3\"]\n</code></pre>"},{"location":"container-workflow/#building-the-image","title":"Building the Image","text":"<pre><code># Build the image\ndocker build -t my-app:latest .\n\n# Test locally (optional)\ndocker run --rm -it my-app:latest --version\n</code></pre>"},{"location":"container-workflow/#multi-stage-builds-recommended-for-smaller-images","title":"Multi-Stage Builds (Recommended for smaller images)","text":"<pre><code># Build stage\nFROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 AS builder\nRUN apt-get update &amp;&amp; apt-get install -y build-essential\n# ... compile dependencies ...\n\n# Runtime stage\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\nCOPY --from=builder /compiled/libs /usr/local/lib\n# ... rest of the Dockerfile ...\n</code></pre>"},{"location":"container-workflow/#step-2-converting-to-singularity","title":"Step 2: Converting to Singularity","text":""},{"location":"container-workflow/#prerequisites","title":"Prerequisites","text":"<p>Ensure Apptainer is installed: <pre><code>apptainer --version  # Should show 1.2.5 or compatible\n</code></pre></p>"},{"location":"container-workflow/#conversion-process","title":"Conversion Process","text":"<pre><code># Create directory for exports\nmkdir -p container-exports\ncd container-exports\n\n# Convert Docker to Singularity sandbox\nAPPTAINER_NOHTTPS=1 apptainer build --sandbox --fakeroot \\\n    my-app.sif docker-daemon://my-app:latest\n\n# Compress for transfer (required for efficient copying)\ntar -czf my-app.tar.gz my-app.sif\n</code></pre> <p>Timing: For an 8GB container, expect: - Conversion: 1-2 minutes - Compression: 2-3 minutes</p>"},{"location":"container-workflow/#step-3-transferring-to-euler","title":"Step 3: Transferring to Euler","text":""},{"location":"container-workflow/#directory-structure","title":"Directory Structure","text":"<p>First, set up your directories on Euler: <pre><code>ssh euler &lt;&lt; 'EOF'\nmkdir -p /cluster/work/rsl/$USER/containers\nmkdir -p /cluster/project/rsl/$USER/results\nmkdir -p /cluster/scratch/$USER/datasets\nEOF\n</code></pre></p>"},{"location":"container-workflow/#transfer-the-container","title":"Transfer the Container","text":"<pre><code># Transfer compressed container\nscp container-exports/my-app.tar.gz \\\n    euler:/cluster/work/rsl/$USER/containers/\n\n# For large transfers, use rsync with resume capability\nrsync -avP container-exports/my-app.tar.gz \\\n    euler:/cluster/work/rsl/$USER/containers/\n</code></pre>"},{"location":"container-workflow/#step-4-running-with-slurm","title":"Step 4: Running with SLURM","text":""},{"location":"container-workflow/#basic-job-script","title":"Basic Job Script","text":"<p>Create <code>job_script.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my-container-job\n#SBATCH --output=logs/job_%j.out\n#SBATCH --error=logs/job_%j.err\n#SBATCH --time=24:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=4G\n#SBATCH --gpus=1\n#SBATCH --tmp=100G\n\n# Load modules\nmodule load eth_proxy\n\n# Job info\necho \"Job started on $(hostname) at $(date)\"\necho \"Job ID: $SLURM_JOB_ID\"\necho \"GPU: $CUDA_VISIBLE_DEVICES\"\n\n# Extract container to local scratch (CRITICAL for performance)\necho \"Extracting container...\"\ntime tar -xzf /cluster/work/rsl/$USER/containers/my-app.tar.gz -C $TMPDIR\n\n# Setup directories\nRESULTS_DIR=\"/cluster/project/rsl/$USER/results/$SLURM_JOB_ID\"\nmkdir -p $RESULTS_DIR\n\n# Run container\necho \"Running application...\"\ntime singularity exec \\\n    --nv \\\n    --bind $RESULTS_DIR:/output \\\n    --bind /cluster/scratch/$USER:/data:ro \\\n    $TMPDIR/my-app.sif \\\n    python3 /workspace/main.py \\\n        --data-dir /data \\\n        --output-dir /output\n\necho \"Job completed at $(date)\"\n</code></pre>"},{"location":"container-workflow/#multi-gpu-job-script","title":"Multi-GPU Job Script","text":"<p>For parallel GPU training:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=multi-gpu-training\n#SBATCH --output=logs/job_%j.out\n#SBATCH --error=logs/job_%j.err\n#SBATCH --time=72:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=32\n#SBATCH --mem-per-cpu=8G\n#SBATCH --gpus=4\n#SBATCH --tmp=200G\n\nmodule load eth_proxy\n\n# Extract container\ntar -xzf /cluster/work/rsl/$USER/containers/my-app.tar.gz -C $TMPDIR\n\n# Run distributed training\nsingularity exec \\\n    --nv \\\n    --bind /cluster/project/rsl/$USER/checkpoints:/checkpoints \\\n    --bind /cluster/scratch/$USER/datasets:/data:ro \\\n    $TMPDIR/my-app.sif \\\n    python3 -m torch.distributed.run \\\n        --nproc_per_node=4 \\\n        train.py --distributed\n</code></pre>"},{"location":"container-workflow/#interactive-development","title":"Interactive Development","text":"<p>For debugging and development:</p> <pre><code># Request interactive session\nsrun --gpus=1 --mem=32G --tmp=50G --pty bash\n\n# Extract and run container interactively\ntar -xzf /cluster/work/rsl/$USER/containers/my-app.tar.gz -C $TMPDIR\nsingularity shell --nv $TMPDIR/my-app.sif\n</code></pre>"},{"location":"container-workflow/#performance-optimization","title":"Performance Optimization","text":""},{"location":"container-workflow/#storage-best-practices","title":"Storage Best Practices","text":"Data Type Location Purpose Containers <code>/cluster/work/rsl/$USER/containers/</code> Long-term storage Results <code>/cluster/project/rsl/$USER/results/</code> Persistent outputs Datasets <code>/cluster/scratch/$USER/</code> Large data, auto-cleaned Working files <code>$TMPDIR</code> Fast local scratch"},{"location":"container-workflow/#timing-benchmarks","title":"Timing Benchmarks","text":"<p>From our testing with 8GB containers:</p> <pre><code>Container extraction to $TMPDIR: 10-15 seconds\nContainer extraction to /cluster/work: 2-5 minutes (avoid!)\nContainer startup overhead: ~2 seconds\nGPU initialization: &lt;1 second\n</code></pre>"},{"location":"container-workflow/#tips-for-optimal-performance","title":"Tips for Optimal Performance","text":"<ol> <li> <p>Always extract to <code>$TMPDIR</code> <pre><code># Good - fast local storage\ntar -xzf /cluster/work/.../container.tar.gz -C $TMPDIR\n\n# Bad - slow network storage\ntar -xzf container.tar.gz -C /cluster/work/...\n</code></pre></p> </li> <li> <p>Pre-stage data when possible <pre><code># Copy frequently accessed data to $TMPDIR\ncp -r /cluster/scratch/$USER/dataset $TMPDIR/\n</code></pre></p> </li> <li> <p>Use read-only binds for input data <pre><code>--bind /cluster/scratch/$USER/data:/data:ro\n</code></pre></p> </li> <li> <p>Monitor resource usage <pre><code># In another terminal while job runs\nssh euler squeue -j $JOBID\nssh $NODE nvidia-smi\n</code></pre></p> </li> </ol>"},{"location":"container-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"container-workflow/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Container extraction is very slow - Ensure you're extracting to <code>$TMPDIR</code> - Check available space: <code>df -h $TMPDIR</code> - Consider using uncompressed tar: <code>tar -cf</code> instead of <code>tar -czf</code></p> <p>GPU not detected in container <pre><code># Check if --nv flag is present\n# Verify CUDA versions match\nsingularity exec --nv container.sif nvidia-smi\n</code></pre></p> <p>Permission denied errors <pre><code># Build with fakeroot\napptainer build --fakeroot ...\n\n# Ensure directories exist and are writable\nmkdir -p /cluster/project/rsl/$USER/results\n</code></pre></p> <p>Out of memory during extraction <pre><code>#SBATCH --tmp=200G  # Increase local scratch\n</code></pre></p>"},{"location":"container-workflow/#debug-commands","title":"Debug Commands","text":"<pre><code># Check job details\nscontrol show job $SLURM_JOB_ID\n\n# Monitor job resource usage\nsstat -j $SLURM_JOB_ID\n\n# View node status\nsinfo -N -l\n\n# Check your disk quotas\nlquota\n</code></pre>"},{"location":"container-workflow/#advanced-topics","title":"Advanced Topics","text":""},{"location":"container-workflow/#container-caching","title":"Container Caching","text":"<p>For frequently used containers, consider keeping extracted versions:</p> <pre><code># One-time extraction (use project for extracted containers)\ntar -xzf /cluster/work/rsl/$USER/containers/container.tar.gz \\\n    -C /cluster/project/rsl/$USER/containers/extracted/\n\n# In job script, just copy\ncp -r /cluster/project/rsl/$USER/containers/extracted/my-app.sif $TMPDIR/\n</code></pre>"},{"location":"container-workflow/#automated-workflows","title":"Automated Workflows","text":"<p>Example automation script:</p> <pre><code>#!/bin/bash\n# build_and_deploy.sh\n\nIMAGE_NAME=\"my-app\"\nVERSION=$(date +%Y%m%d-%H%M%S)\n\n# Build\ndocker build -t ${IMAGE_NAME}:${VERSION} .\n\n# Convert\napptainer build --fakeroot ${IMAGE_NAME}-${VERSION}.sif \\\n    docker-daemon://${IMAGE_NAME}:${VERSION}\n\n# Compress and transfer\ntar -czf ${IMAGE_NAME}-${VERSION}.tar.gz ${IMAGE_NAME}-${VERSION}.sif\nscp ${IMAGE_NAME}-${VERSION}.tar.gz euler:/cluster/work/rsl/$USER/containers/\n\n# Create job script from template\nsed \"s/VERSION/${VERSION}/g\" job_template.sh &gt; job_${VERSION}.sh\n\n# Submit\nssh euler \"cd /cluster/work/rsl/$USER &amp;&amp; sbatch job_${VERSION}.sh\"\n</code></pre>"},{"location":"container-workflow/#test-scripts-and-files","title":"\ud83e\uddea Test Scripts and Files","text":"<p>Complete test files for the container workflow:</p> <ul> <li>Dockerfile - Test Docker image with CUDA support</li> <li>hello_cluster.py - Python test script for containers</li> <li>test_job_project.sh - Complete container job example</li> <li>test_container_extraction.sh - Test container extraction timing</li> </ul> <p>To test the complete workflow: <pre><code># 1. Build Docker image locally\ndocker build -t euler-test:latest -f Dockerfile .\n\n# 2. Convert to Singularity\napptainer build --sandbox --fakeroot euler-test.sif docker-daemon://euler-test:latest\n\n# 3. Compress and transfer\ntar -czf euler-test.tar.gz euler-test.sif\nscp euler-test.tar.gz euler:/cluster/work/rsl/$USER/containers/\n\n# 4. Submit test job\nssh euler\nsbatch test_job_project.sh\n</code></pre></p> <p>Back to Home | View Scripts | Troubleshooting</p>"},{"location":"data-management/","title":"Data Management on Euler","text":"<p>Effective data management is critical when working on the Euler Cluster, particularly for machine learning workflows that involve large datasets and model outputs. This section explains the available storage options and their proper usage.</p>"},{"location":"data-management/#home-directory-clusterhomeuser","title":"\ud83d\udcc1 Home Directory (<code>/cluster/home/$USER</code>)","text":"<ul> <li>Quota: 45 GB  </li> <li>Inodes: ~450,000 files  </li> <li>Persistence: Permanent (not purged)</li> <li>Use Case: Ideal for storing source code, small configuration files, scripts, and lightweight development tools.</li> </ul>"},{"location":"data-management/#scratch-directory-clusterscratchuser-or-scratch","title":"\u26a1 Scratch Directory (<code>/cluster/scratch/$USER</code> or <code>$SCRATCH</code>)","text":"<ul> <li>Quota: 2.5 TB  </li> <li>Inodes: 1 M  </li> <li>Persistence: Temporary (data is deleted if not accessed for ~15 days)</li> <li>Use Case: For storing datasets and temporary training outputs.</li> <li>Recommended Dataset storage format: Use tar/zip/HDF5/WebDataset.</li> </ul>"},{"location":"data-management/#project-directory-clusterprojectrsluser","title":"\ud83d\udce6 Project Directory (<code>/cluster/project/rsl/$USER</code>)","text":"<ul> <li>Quota: \u2264 75 GB  </li> <li>Inodes: ~2.5 M  </li> <li>Use Case: Conda environments, software packages</li> </ul>"},{"location":"data-management/#work-directory-clusterworkrsluser","title":"\ud83d\udcc2 Work Directory (<code>/cluster/work/rsl/$USER</code>)","text":"<ul> <li>Quota: \u2264 200 GB  </li> <li>Inodes: ~50,000  </li> <li>Use Case: Saving results, large output files, tar files, singularity images. Avoid storing too many small files.</li> </ul> <p>In exceptional cases we can approve more storage space. For this, ask your supervisor to contact <code>patelm@ethz.ch</code>.</p>"},{"location":"data-management/#local-scratch-directory-tmpdir","title":"\ud83d\udcc2 Local Scratch Directory (<code>$TMPDIR</code>)","text":"<ul> <li>Quota: upto 800 GB  </li> <li>Inodes: Very High </li> <li>Use Case: Datasets and containers for a training run. </li> </ul>"},{"location":"data-management/#quota-violations","title":"\u2757 Quota Violations:","text":"<ul> <li>You shall receive an email if you violate any of the above limits. </li> <li>You can type <code>lquota</code> in the terminal to check your used storage space for <code>Home</code> and <code>Scratch</code> directories. </li> <li>For usage of <code>Project</code> and <code>Work</code> directories you can run:     <pre><code>(head -n 5 &amp;&amp; grep -w $USER) &lt; /cluster/work/rsl/.rsl_user_data_usage.txt\n(head -n 5 &amp;&amp; grep -w $USER) &lt; /cluster/project/rsl/.rsl_user_data_usage.txt\n</code></pre>    Note: This wont show the per-user quota limit which is enforced by RSL ! Refer to the table below for the quota limits.</li> </ul>"},{"location":"data-management/#faq-what-is-the-difference-between-the-project-and-work-directories-and-why-is-it-necessary-to-make-use-of-both","title":"\ud83c\udfaf FAQ: What is the difference between the <code>Project</code> and <code>Work</code> Directories and why is it necessary to make use of both?","text":"<p>Basically, both <code>Project</code> and <code>Work</code> are persistent storages (meaning the data is not deleted automatically); however, the use cases are different. When you have lots of small files, for example, conda environments, you should store them in the <code>Project</code> directory as it has a higher capacity for # of inodes. On the other hand, when you have larger files such as model checkpoints, singularity containers and results you should store them in the <code>Work</code> directory as the storage capacity is higher.</p>"},{"location":"data-management/#faq-what-is-local-scratch-directory-tmpdir","title":"\ud83c\udfaf FAQ: What is Local Scratch Directory (<code>$TMPDIR</code>) ?","text":"<p>Whenever you run a compute job, you can also ask for a certain amount of local scratch space (<code>$TMPDIR</code>) which allocates space on a local hard drive. The main advantage of the local scratch is, that it is located directly inside the compute nodes and not attached via the network. Thus it is highly recommended to copy over your singularity container / datasets to <code>$TMPDIR</code> and then use that for the trainings. Detailed workflows for the trainings are provided later in this guide.</p>"},{"location":"data-management/#summary-table-of-storage-locations","title":"\ud83d\udcca Summary Table of Storage Locations","text":"Storage Location Max Inodes Max Size per User Purged Recommended Use Case <code>/cluster/home/$USER</code> ~450,000 45 GB No Code, config, small files <code>/cluster/scratch/$USER</code> 1 M 2.5 TB Yes (older than 15 days) Datasets, training data, temporary usage <code>/cluster/project/rsl/$USER</code> 2.5 M 75 GB No Conda envs, software packages <code>/cluster/work/rsl/$USER</code> 50,000 200 GB No Large result files, model checkpoints, Singularity containers, <code>$TMPDIR</code> very high Upto 800 GB Yes (at end of job) Training Datasets, Singularity Images"},{"location":"data-management/#test-scripts","title":"\ud83e\uddea Test Scripts","text":"<p>To verify your storage setup and check quotas:</p> <ul> <li>test_storage_quotas.sh - Checks all storage paths, quotas, and creates missing directories</li> </ul> <p>Submit as a job to test <code>$TMPDIR</code>: <pre><code>sbatch test_storage_quotas.sh\n</code></pre></p>"},{"location":"data-management/#best-practices","title":"\ud83d\udca1 Best Practices","text":"<ol> <li>Use the right storage for the right purpose - Don't waste home directory space on large files</li> <li>Compress datasets - Use tar/zip to reduce inode usage</li> <li>Clean up regularly - Remove old data from scratch before it's auto-deleted</li> <li>Monitor your usage - Check quotas regularly with <code>lquota</code></li> <li>Use <code>$TMPDIR</code> for active jobs - Copy data to local scratch for faster I/O during computation</li> </ol>"},{"location":"getting-started/","title":"Getting Started with Euler","text":"<p>This guide helps new users access and begin working on the Euler Cluster at ETH Zurich, specifically for members of the RSL group (es_hutter).</p>"},{"location":"getting-started/#table-of-contents","title":"\ud83d\udccc Table of Contents","text":"<ol> <li>Access Requirements</li> <li>Connecting to Euler via SSH</li> <li>Basic Login</li> <li>Setting Up SSH Keys</li> <li>Using an SSH Config File</li> <li>Verifying Access to the RSL Shareholder Group</li> </ol>"},{"location":"getting-started/#access-requirements","title":"\u2705 Access Requirements","text":"<p>In order to get access to the cluster, kindly fill up the following form. If you are a member of RSL, directly message Manthan Patel to add you to the cluster. The access is approved twice a week (Tuesdays and Fridays).</p> <p>Before proceeding, make sure you have:</p> <ul> <li>A valid nethz username and password (ETH Zurich credentials)</li> <li>Access to a terminal (Linux/macOS or Git Bash on Windows)</li> <li>(Optional) Some familiarity with command-line tools</li> </ul>"},{"location":"getting-started/#connecting-to-euler-via-ssh","title":"\ud83d\udd10 Connecting to Euler via SSH","text":"<p>You'll connect to Euler using the Secure Shell (SSH) protocol. This allows you to log into a remote machine securely from your local computer.</p>"},{"location":"getting-started/#basic-login","title":"Basic Login","text":"<p>To log into the Euler cluster, open a terminal and type:</p> <pre><code>ssh &lt;your_nethz_username&gt;@euler.ethz.ch\n</code></pre> <p>Replace <code>&lt;your_nethz_username&gt;</code> with your actual ETH Zurich login.</p> <p>You will be asked to enter your ETH Zurich password. If the login is successful, you'll be connected to a login node on the Euler cluster.</p>"},{"location":"getting-started/#setting-up-ssh-keys-recommended","title":"Setting Up SSH Keys (Recommended)","text":"<p>To avoid typing your password every time and to increase security, it is recommended to use SSH key-based authentication.</p>"},{"location":"getting-started/#step-by-step-instructions","title":"Step-by-Step Instructions:","text":"<ol> <li>Generate an SSH key pair on your local machine (if not already created):</li> </ol> <pre><code>ssh-keygen -t ed25519 -C \"&lt;your_email&gt;@ethz.ch\"\n</code></pre> <ul> <li>Press Enter to accept the default file location (usually <code>~/.ssh/id_ed25519</code>).</li> <li> <p>When prompted for a passphrase, you can choose to set one or leave it empty.</p> </li> <li> <p>Copy your public key to Euler using this command:</p> </li> </ul> <pre><code>ssh-copy-id &lt;your_nethz_username&gt;@euler.ethz.ch\n</code></pre> <ul> <li>You'll be asked to enter your ETH password one last time.</li> <li>This command installs your public key in the <code>~/.ssh/authorized_keys</code> file on Euler.</li> </ul> <p>Now, you should be able to log in without typing your password.</p>"},{"location":"getting-started/#using-an-ssh-config-file","title":"Using an SSH Config File","text":"<p>To make your SSH workflow easier, especially if you frequently access Euler, create or edit the <code>~/.ssh/config</code> file on your local machine.</p>"},{"location":"getting-started/#example-configuration","title":"Example Configuration:","text":"<pre><code>Host euler\n  HostName euler.ethz.ch\n  User &lt;your_nethz_username&gt;\n  Compression yes\n  ForwardX11 yes\n  IdentityFile ~/.ssh/id_ed25519\n</code></pre> <ul> <li>Replace <code>&lt;your_nethz_username&gt;</code> with your actual ETH username.</li> <li>Save and close the file.</li> </ul> <p>Now, instead of typing the full SSH command, you can simply connect using:</p> <pre><code>ssh euler\n</code></pre>"},{"location":"getting-started/#verifying-access-to-the-rsl-shareholder-group","title":"\ud83e\uddfe Verifying Access to the RSL Shareholder Group","text":"<p>Once you are logged into the Euler cluster, it's important to confirm that you have been added to the appropriate shareholder group. This ensures you can access the computing resources allocated to your research group (in this case, the RSL group).</p>"},{"location":"getting-started/#quick-setup-recommended","title":"\ud83d\ude80 Quick Setup (Recommended)","text":"<p>We provide a setup script that automatically verifies your RSL group membership and creates all necessary directories:</p> <pre><code># Download and run the setup script\nwget https://raw.githubusercontent.com/leggedrobotics/euler-cluster-guide/main/docs/scripts/getting-started/test_group_membership.sh\nbash test_group_membership.sh\n</code></pre> <p>This script will: - \u2705 Check your RSL group membership - \u2705 Create all required directories in project and work partitions - \u2705 Display your storage quotas - \u2705 Verify everything is set up correctly</p>"},{"location":"getting-started/#manual-setup-alternative","title":"\ud83d\udd0d Manual Setup (Alternative)","text":"<p>If you prefer to set up manually:</p> <ol> <li>While connected to Euler (after logging in via SSH), run the following command in the terminal:</li> </ol> <pre><code>my_share_info\n</code></pre> <ol> <li>If everything is correctly set up, you should see output similar to the following:</li> </ol> <pre><code>You are a member of the es_hutter shareholder group on Euler.\n</code></pre> <ol> <li> <p>This message confirms that you are part of the <code>es_hutter</code> group, which is the shareholder group for the RSL lab.</p> </li> <li> <p>Create your user directories for storage by using the following command:    <pre><code>mkdir -p /cluster/project/rsl/$USER\nmkdir -p /cluster/work/rsl/$USER\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#if-you-do-not-see-this-message","title":"\u2757 If You Do NOT See This Message:","text":"<ul> <li>Double-check with your supervisor whether you've been added to the group.</li> <li>It may take a few hours after being added for the change to propagate.</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Once you have verified your access: - Learn about Data Management on Euler - Set up Python Environments - Start Computing with interactive sessions or batch jobs</p>"},{"location":"python-environments/","title":"Python Environments and ML Training","text":"<p>This guide covers setting up Python environments on Euler and provides a complete machine learning training workflow.</p>"},{"location":"python-environments/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Setting Up Miniconda</li> <li>Managing Environments</li> <li>ML Training Workflow</li> <li>Performance Tips</li> </ol>"},{"location":"python-environments/#setting-up-miniconda","title":"\ud83d\udc0d Setting Up Miniconda","text":"<p>Python virtual environments are commonly used on Euler for managing dependencies in a clean, reproducible way. One of the best tools for this purpose is Miniconda, a lightweight version of Anaconda.</p>"},{"location":"python-environments/#installing-miniconda3","title":"\ud83d\udce6 Installing Miniconda3","text":"<p>To install Miniconda3, follow these steps:</p> <pre><code># Create a directory for Miniconda\nmkdir -p /cluster/project/rsl/$USER/miniconda3\n\n# Download the installer\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /cluster/project/rsl/$USER/miniconda3/miniconda.sh\n\n# Run the installer silently\nbash /cluster/project/rsl/$USER/miniconda3/miniconda.sh -b -u -p /cluster/project/rsl/$USER/miniconda3/\n\n# Clean up the installer\nrm /cluster/project/rsl/$USER/miniconda3/miniconda.sh\n\n# Initialize conda for bash\n/cluster/project/rsl/$USER/miniconda3/bin/conda init bash\n\n# Prevent auto-activation of the base environment\nconda config --set auto_activate_base false\n</code></pre>"},{"location":"python-environments/#avoid-installing-in-home-directory","title":"\ud83d\udeab Avoid Installing in Home Directory","text":"<p>Installing Miniconda in your home directory (<code>~/miniconda3</code>) is not recommended due to storage and inode limits.</p> <p>Instead, install it in your project directory:</p> <pre><code>/cluster/project/rsl/$USER/miniconda3\n</code></pre>"},{"location":"python-environments/#if-youve-already-installed-miniconda-in-your-home-directory","title":"\u2705 If you've already installed Miniconda in your home directory:","text":"<p>You can move it to your project directory and create a symbolic link:</p> <pre><code>mv ~/miniconda3 /cluster/project/rsl/$USER/\nln -s /cluster/project/rsl/$USER/miniconda3 ~/miniconda3\n</code></pre> <p>This way, conda commands referencing <code>~/miniconda3</code> will still work, while the files reside in a directory with more storage and inode capacity.</p>"},{"location":"python-environments/#managing-environments","title":"\ud83e\uddea Managing Environments","text":""},{"location":"python-environments/#creating-a-sample-conda-environment","title":"Creating a Sample Conda Environment","text":"<p>Once Miniconda is installed and configured, you can create a new conda environment like this:</p> <pre><code>conda create -n myenv python=3.10\n</code></pre> <p>To activate the environment:</p> <pre><code>conda activate myenv\n</code></pre> <p>You can install packages as needed:</p> <pre><code>conda install numpy pandas matplotlib\n</code></pre> <p>To deactivate the environment:</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"python-environments/#best-practices","title":"\ud83d\udca1 Best Practices","text":"<ul> <li>Always install environments in <code>/cluster/project/rsl/$USER/miniconda3</code> to avoid inode overflow in your home directory.</li> <li>Use a compute node for the installation process, so you can make use of the bandwidth and the I/O available there, but be sure to request more than an hour for your session, so the progress is not lost if there are a lot of packages to install.</li> <li>Export the list of installed packages as soon as you confirm that an environment is working as expected. Set a mnemonic file name for that list, and save it in a secure place, in case you need to install the environment again.</li> </ul>"},{"location":"python-environments/#ml-training-workflow","title":"\ud83d\ude82 ML Training Workflow","text":"<p>This section provides a complete end-to-end workflow for machine learning training on Euler using conda environments.</p>"},{"location":"python-environments/#complete-workflow-overview","title":"\ud83d\udccb Complete Workflow Overview","text":"<ol> <li>Set up conda environment with dependencies</li> <li>Prepare and stage datasets</li> <li>Develop training script with checkpointing</li> <li>Submit batch job for training</li> <li>Monitor progress and collect results</li> </ol>"},{"location":"python-environments/#1-environment-setup","title":"1\ufe0f\u20e3 Environment Setup","text":""},{"location":"python-environments/#create-project-structure","title":"Create Project Structure","text":"<pre><code>ssh euler\ncd /cluster/home/$USER\nmkdir -p ml_project/{scripts,data,configs,logs}\ncd ml_project\n</code></pre>"},{"location":"python-environments/#install-conda-environment","title":"Install Conda Environment","text":"<pre><code># Request interactive session for installation\nsrun --time=1:00:00 --mem=16G --cpus-per-task=8 --pty bash\n\n# Create environment with specific Python version\nconda create -n ml_training python=3.10 -y\n\n# Activate environment\nconda activate ml_training\n\n# Install PyTorch with CUDA support\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y\n\n# Install additional ML packages\nconda install numpy pandas scikit-learn matplotlib seaborn jupyterlab -y\npip install wandb tensorboard transformers datasets\n\n# Save environment\nconda env export &gt; environment.yml\n</code></pre>"},{"location":"python-environments/#2-data-preparation-script","title":"2\ufe0f\u20e3 Data Preparation Script","text":"<p>Create <code>prepare_data.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Prepare and preprocess dataset for training.\"\"\"\n\nimport os\nimport argparse\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport h5py\n\ndef prepare_dataset(args):\n    \"\"\"Download and preprocess dataset.\"\"\"\n\n    # Define transforms\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n\n    # Download dataset\n    print(f\"Downloading dataset to {args.data_dir}\")\n    train_dataset = datasets.CIFAR10(\n        root=args.data_dir,\n        train=True,\n        download=True,\n        transform=transform\n    )\n\n    val_dataset = datasets.CIFAR10(\n        root=args.data_dir,\n        train=False,\n        download=True,\n        transform=transform\n    )\n\n    # Convert to HDF5 for faster loading\n    if args.convert_hdf5:\n        print(\"Converting to HDF5 format...\")\n\n        train_file = os.path.join(args.output_dir, 'train.h5')\n        val_file = os.path.join(args.output_dir, 'val.h5')\n\n        # Save training data\n        with h5py.File(train_file, 'w') as f:\n            images = f.create_dataset('images', (len(train_dataset), 3, 224, 224), dtype='f')\n            labels = f.create_dataset('labels', (len(train_dataset),), dtype='i')\n\n            for idx, (img, label) in enumerate(train_dataset):\n                if idx % 1000 == 0:\n                    print(f\"Processing training sample {idx}/{len(train_dataset)}\")\n                images[idx] = img.numpy()\n                labels[idx] = label\n\n        print(f\"Saved training data to {train_file}\")\n\n    return len(train_dataset), len(val_dataset)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data-dir', type=str, required=True)\n    parser.add_argument('--output-dir', type=str, required=True)\n    parser.add_argument('--convert-hdf5', action='store_true')\n    args = parser.parse_args()\n\n    os.makedirs(args.output_dir, exist_ok=True)\n    train_size, val_size = prepare_dataset(args)\n    print(f\"Dataset prepared: {train_size} training, {val_size} validation samples\")\n</code></pre>"},{"location":"python-environments/#3-training-script-with-checkpointing","title":"3\ufe0f\u20e3 Training Script with Checkpointing","text":"<p>Create a comprehensive training script <code>train.py</code> that includes: - Model initialization - Data loading - Training loop with validation - Checkpointing - Logging to wandb and tensorboard</p> <p>[Full training script available in the complete example below]</p>"},{"location":"python-environments/#4-batch-job-script","title":"4\ufe0f\u20e3 Batch Job Script","text":"<p>Create <code>train_job.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=ml-training\n#SBATCH --output=logs/train_%j.out\n#SBATCH --error=logs/train_%j.err\n#SBATCH --time=24:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=16\n#SBATCH --mem-per-cpu=4G\n#SBATCH --gpus=1\n#SBATCH --tmp=100G\n\n# Load modules\nmodule load eth_proxy\n\n# Job information\necho \"=========================================\"\necho \"SLURM Job ID: $SLURM_JOB_ID\"\necho \"Running on: $(hostname)\"\necho \"Starting at: $(date)\"\necho \"GPU: $CUDA_VISIBLE_DEVICES\"\necho \"=========================================\"\n\n# Set up environment variables\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport WANDB_DIR=/cluster/scratch/$USER/wandb\nexport WANDB_CACHE_DIR=$WANDB_DIR/cache\nexport WANDB_CONFIG_DIR=$WANDB_DIR/config\nmkdir -p $WANDB_DIR\n\n# Activate conda environment\nsource /cluster/project/rsl/$USER/miniconda3/bin/activate\nconda activate ml_training\n\n# Copy dataset to local scratch for faster I/O\necho \"Copying dataset to local scratch...\"\ncp -r /cluster/scratch/$USER/datasets/cifar10 $TMPDIR/\necho \"Dataset copied successfully\"\n\n# Set paths\nDATA_DIR=$TMPDIR/cifar10\nOUTPUT_DIR=/cluster/project/rsl/$USER/results/$SLURM_JOB_ID\nCHECKPOINT_DIR=/cluster/project/rsl/$USER/checkpoints/$SLURM_JOB_ID\n\nmkdir -p $OUTPUT_DIR $CHECKPOINT_DIR\n\n# Run training\ncd /cluster/home/$USER/ml_project\npython train.py \\\n    --data-dir $DATA_DIR \\\n    --output-dir $OUTPUT_DIR \\\n    --checkpoint-dir $CHECKPOINT_DIR \\\n    --model resnet18 \\\n    --epochs 100 \\\n    --batch-size 128 \\\n    --lr 0.001 \\\n    --num-workers 8 \\\n    --log-interval 50 \\\n    --save-interval 10\n\n# Copy final results\necho \"Copying results...\"\ncp -r $OUTPUT_DIR/* /cluster/project/rsl/$USER/final_results/\n\necho \"Job completed at $(date)\"\n</code></pre>"},{"location":"python-environments/#5-monitoring-and-results-collection","title":"5\ufe0f\u20e3 Monitoring and Results Collection","text":""},{"location":"python-environments/#monitor-training-progress","title":"Monitor Training Progress","text":"<pre><code># Check job status\nsqueue -u $USER\n\n# Watch GPU utilization\nssh euler\nsqueue -u $USER -o \"%.18i %.9P %.8j %.8u %.2t %.10M %.6D %N\"\n# Get node name, then:\nssh &lt;node_name&gt; nvidia-smi -l 1\n\n# View training logs in real-time\ntail -f logs/train_*.out\n\n# Check tensorboard logs\ncd /cluster/project/rsl/$USER/results/&lt;job_id&gt;/tensorboard\ntensorboard --logdir . --port 8888\n# Then create SSH tunnel: ssh -L 8888:localhost:8888 euler\n</code></pre>"},{"location":"python-environments/#performance-tips","title":"\ud83d\udcca Performance Tips","text":""},{"location":"python-environments/#1-data-loading-optimization","title":"1. Data Loading Optimization","text":"<ul> <li>Use local scratch (<code>$TMPDIR</code>) for datasets</li> <li>Convert to efficient formats (HDF5, TFRecord)</li> <li>Use multiple workers for data loading</li> </ul>"},{"location":"python-environments/#2-gpu-utilization","title":"2. GPU Utilization","text":"<ul> <li>Monitor with <code>nvidia-smi</code> during training</li> <li>Increase batch size if GPU memory allows</li> <li>Use mixed precision training (AMP) for speedup</li> </ul>"},{"location":"python-environments/#3-checkpointing-strategy","title":"3. Checkpointing Strategy","text":"<ul> <li>Save checkpoints to persistent storage</li> <li>Keep only best and recent checkpoints</li> <li>Resume from checkpoints after job time limit</li> </ul>"},{"location":"python-environments/#4-distributed-training","title":"4. Distributed Training","text":"<ul> <li>Use multiple GPUs with <code>torch.distributed</code></li> <li>Scale batch size with number of GPUs</li> <li>Adjust learning rate accordingly</li> </ul>"},{"location":"python-environments/#complete-workflow-example","title":"\ud83c\udfaf Complete Workflow Example","text":"<pre><code># 1. Prepare environment and data\nssh euler\ncd /cluster/home/$USER/ml_project\nsbatch prepare_data_job.sh\n\n# 2. Test with small run\nsrun --gpus=1 --mem=16G --time=0:30:00 --pty bash\npython train.py --epochs 2 --batch-size 32 ...\n\n# 3. Submit full training\nsbatch train_job.sh\n\n# 4. Monitor progress\nwatch -n 60 squeue -u $USER\ntail -f logs/train_*.out\n\n# 5. Analyze results\npython analyze_results.py --results-dir /cluster/project/rsl/$USER/results/&lt;job_id&gt;\n</code></pre>"},{"location":"python-environments/#test-scripts","title":"\ud83e\uddea Test Scripts","text":"<p>We provide complete test scripts to verify the ML workflow:</p> <ul> <li>fake_train.py - Simulated ML training script for testing</li> <li>test_full_training_job.sh - Complete ML training job test</li> </ul> <p>To test the full workflow: <pre><code># Copy scripts to your project\ncp fake_train.py /cluster/home/$USER/\ncp test_full_training_job.sh /cluster/home/$USER/\n\n# Submit the test job\nsbatch test_full_training_job.sh\n</code></pre></p>"},{"location":"python-environments/#sample-training-script","title":"\ud83d\udcdd Sample Training Script","text":"<p>For a complete training script example with all features (checkpointing, logging, etc.), see the Computing Guide or download our template from the Scripts Library.</p>"},{"location":"scripts/","title":"Scripts Library","text":"<p>Ready-to-use scripts for the Euler cluster, organized by workflow section. All scripts have been tested on the Euler cluster with the RSL group allocation.</p>"},{"location":"scripts/#scripts-organization","title":"\ud83d\udcc1 Scripts Organization","text":"<pre><code>scripts/\n\u251c\u2500\u2500 getting-started/       # Initial setup scripts\n\u251c\u2500\u2500 data-management/       # Storage and quota management\n\u251c\u2500\u2500 python-environments/   # ML training examples\n\u251c\u2500\u2500 computing-guide/       # Job submission templates\n\u2514\u2500\u2500 container-workflow/    # Container deployment scripts\n</code></pre>"},{"location":"scripts/#getting-started-scripts","title":"\ud83d\ude80 Getting Started Scripts","text":""},{"location":"scripts/#setup-verification","title":"Setup Verification","text":"<p>test_group_membership.sh</p> <p>Verifies RSL group membership and creates all necessary directories: <pre><code>wget https://raw.githubusercontent.com/leggedrobotics/euler-cluster-guide/main/docs/scripts/getting-started/test_group_membership.sh\nbash test_group_membership.sh\n</code></pre></p>"},{"location":"scripts/#data-management-scripts","title":"\ud83d\udcbe Data Management Scripts","text":""},{"location":"scripts/#storage-quota-check","title":"Storage Quota Check","text":"<p>test_storage_quotas.sh</p> <p>Comprehensive storage verification script that: - Checks all storage paths and creates missing directories - Displays current usage and quotas - Tests <code>$TMPDIR</code> functionality in job context</p>"},{"location":"scripts/#python-ml-training-scripts","title":"\ud83d\udc0d Python &amp; ML Training Scripts","text":""},{"location":"scripts/#ml-training-example","title":"ML Training Example","text":"<p>fake_train.py | test_full_training_job.sh</p> <p>Complete ML training workflow example including: - Simulated training with checkpointing - Progress tracking and logging - Resource monitoring - Proper use of local scratch for data</p>"},{"location":"scripts/#computing-scripts","title":"\ud83d\udcbb Computing Scripts","text":""},{"location":"scripts/#basic-job-templates","title":"Basic Job Templates","text":"<ul> <li>test_cpu_job.sh - Basic CPU job submission</li> <li>test_gpu_job.sh - GPU allocation test</li> <li>test_gpu_specific.sh - Request specific GPU type (RTX 4090)</li> <li>test_array_job.sh - Array job for parameter sweeps</li> </ul>"},{"location":"scripts/#advanced-templates","title":"Advanced Templates","text":""},{"location":"scripts/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=multi-gpu-train\n#SBATCH --output=logs/job_%j.out\n#SBATCH --error=logs/job_%j.err\n#SBATCH --time=72:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=32\n#SBATCH --mem-per-cpu=8G\n#SBATCH --gpus=4\n#SBATCH --tmp=200G\n\nmodule load eth_proxy\n\n# Extract container to local scratch\ntar -xf /cluster/work/rsl/$USER/containers/training.tar -C $TMPDIR\n\n# Run distributed training\nsingularity exec \\\n    --nv \\\n    --bind /cluster/project/rsl/$USER/checkpoints:/checkpoints \\\n    --bind /cluster/scratch/$USER/datasets:/data:ro \\\n    $TMPDIR/training.sif \\\n    python3 -m torch.distributed.run \\\n        --nproc_per_node=4 \\\n        train.py --distributed\n</code></pre>"},{"location":"scripts/#interactive-development-session","title":"Interactive Development Session","text":"<pre><code># Request interactive GPU session\nsrun --gpus=1 --mem=32G --tmp=50G --time=2:00:00 --pty bash\n\n# In the session, extract and use container\ntar -xf /cluster/work/rsl/$USER/containers/dev.tar -C $TMPDIR\n\nsingularity shell --nv \\\n    --bind /cluster/project/rsl/$USER:/project \\\n    --bind /cluster/scratch/$USER:/data \\\n    $TMPDIR/dev.sif\n</code></pre>"},{"location":"scripts/#container-workflow-scripts","title":"\ud83d\udce6 Container Workflow Scripts","text":""},{"location":"scripts/#container-test-suite","title":"Container Test Suite","text":"<ul> <li>Dockerfile - GPU-enabled Docker image with CUDA 11.8</li> <li>hello_cluster.py - GPU functionality test</li> <li>test_job_project.sh - Complete container job</li> <li>test_container_extraction.sh - Extraction timing test</li> </ul>"},{"location":"scripts/#build-and-deploy-helper","title":"Build and Deploy Helper","text":"<pre><code>#!/bin/bash\n# build_and_deploy.sh\n\nset -e\n\nIMAGE_NAME=${1:-\"my-app\"}\nVERSION=$(date +%Y%m%d-%H%M%S)\n\necho \"Building ${IMAGE_NAME}:${VERSION}...\"\n\n# Build Docker image\ndocker build -t ${IMAGE_NAME}:${VERSION} .\n\n# Convert to Singularity\necho \"Converting to Singularity...\"\napptainer build --sandbox --fakeroot \\\n    ${IMAGE_NAME}-${VERSION}.sif \\\n    docker-daemon://${IMAGE_NAME}:${VERSION}\n\n# Compress\necho \"Compressing...\"\ntar -czf ${IMAGE_NAME}-${VERSION}.tar.gz ${IMAGE_NAME}-${VERSION}.sif\n\n# Transfer\necho \"Transferring to Euler...\"\nscp ${IMAGE_NAME}-${VERSION}.tar.gz \\\n    euler:/cluster/work/rsl/$USER/containers/\n\necho \"Done! Container available as ${IMAGE_NAME}-${VERSION}.tar.gz\"\n</code></pre>"},{"location":"scripts/#utility-scripts","title":"\ud83d\udd27 Utility Scripts","text":""},{"location":"scripts/#job-resource-monitor","title":"Job Resource Monitor","text":"<pre><code>#!/bin/bash\n# monitor_job.sh\n\nJOB_ID=$1\n\nif [ -z \"$JOB_ID\" ]; then\n    echo \"Usage: $0 &lt;job_id&gt;\"\n    exit 1\nfi\n\nwhile true; do\n    clear\n    echo \"=== Job $JOB_ID Status ===\"\n    squeue -j $JOB_ID\n\n    echo -e \"\\n=== Resource Usage ===\"\n    sstat -j $JOB_ID --format=JobID,MaxRSS,MaxDiskRead,MaxDiskWrite\n\n    # Get node name and check GPU\n    NODE=$(squeue -j $JOB_ID -h -o %N)\n    if [ ! -z \"$NODE\" ]; then\n        echo -e \"\\n=== GPU Usage on $NODE ===\"\n        ssh $NODE nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv\n    fi\n\n    sleep 5\ndone\n</code></pre>"},{"location":"scripts/#batch-job-status-check","title":"Batch Job Status Check","text":"<pre><code>#!/bin/bash\n# check_jobs.sh\n\necho \"=== Your Current Jobs ===\"\nsqueue -u $USER --format=\"%.18i %.9P %.30j %.8u %.2t %.10M %.6D %R\"\n\necho -e \"\\n=== Recently Completed Jobs ===\"\nsacct -u $USER --starttime=$(date -d '1 day ago' +%Y-%m-%d) \\\n    --format=JobID,JobName,State,ExitCode,Elapsed,MaxRSS\n\necho -e \"\\n=== Storage Usage ===\"\nlquota\n</code></pre>"},{"location":"scripts/#download-scripts","title":"\ud83d\udce5 Download Scripts","text":"<p>Clone the entire repository to get all scripts:</p> <pre><code>git clone https://github.com/leggedrobotics/euler-cluster-guide.git\ncd euler-cluster-guide/docs/scripts\n\n# Make all scripts executable\nfind . -name \"*.sh\" -type f -exec chmod +x {} \\;\n</code></pre> <p>Or download individual scripts: <pre><code># Example: Download the GPU test job\nwget https://raw.githubusercontent.com/leggedrobotics/euler-cluster-guide/main/docs/scripts/computing-guide/test_gpu_job.sh\n</code></pre></p> <p>Back to Home | Computing Guide | Container Workflow | Troubleshooting</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>This page covers common issues and solutions when working with containers on the Euler cluster.</p>"},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"troubleshooting/#container-build-issues","title":"Container Build Issues","text":""},{"location":"troubleshooting/#docker-daemon-not-running","title":"Docker daemon not running","text":"<p><pre><code>Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n</code></pre> Solution: Start Docker service <pre><code>sudo systemctl start docker\n# or on macOS:\nopen -a Docker\n</code></pre></p>"},{"location":"troubleshooting/#apptainersingularity-not-found","title":"Apptainer/Singularity not found","text":"<p><pre><code>bash: apptainer: command not found\n</code></pre> Solution: Install Apptainer (v1.2.5 recommended) <pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install -y apptainer\n\n# From source (recommended for version control)\nwget https://github.com/apptainer/apptainer/releases/download/v1.2.5/apptainer-1.2.5.tar.gz\n</code></pre></p>"},{"location":"troubleshooting/#transfer-issues","title":"Transfer Issues","text":""},{"location":"troubleshooting/#connection-timeout-during-scp","title":"Connection timeout during SCP","text":"<p><pre><code>ssh: connect to host euler.ethz.ch port 22: Connection timed out\n</code></pre> Solution:  - Check VPN connection if off-campus - Use ETH proxy: <code>module load eth_proxy</code> - Try alternative transfer method: <pre><code># Use rsync with resume capability\nrsync -avP --append-verify container.tar euler:/cluster/work/rsl/$USER/containers/\n</code></pre></p>"},{"location":"troubleshooting/#insufficient-space-during-transfer","title":"Insufficient space during transfer","text":"<p><pre><code>scp: write: No space left on device\n</code></pre> Solution: Check quotas and clean up <pre><code># Check your quotas\nlquota\n\n# Clean old containers\nrm /cluster/work/rsl/$USER/containers/old-*.tar\n</code></pre></p>"},{"location":"troubleshooting/#slurm-job-issues","title":"SLURM Job Issues","text":""},{"location":"troubleshooting/#job-stays-pending","title":"Job stays pending","text":"<p><pre><code>JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n12345    gpu.4h container   user PD       0:00      1 (Resources)\n</code></pre> Solution:  - Reduce resource requirements - Check available resources: <code>sinfo -o \"%P %a %l %D %G\"</code> - Use different partition: <code>#SBATCH --partition=gpu.24h</code></p>"},{"location":"troubleshooting/#container-extraction-fails","title":"Container extraction fails","text":"<p><pre><code>tar: my-app.sif: Cannot open: No such file or directory\n</code></pre> Solution: Verify container path <pre><code># List available containers\nls -la /cluster/work/rsl/$USER/containers/\n\n# Check if extraction completed\nls -la $TMPDIR/\n</code></pre></p>"},{"location":"troubleshooting/#gpu-not-detected","title":"GPU not detected","text":"<p><pre><code>CUDA available: False\n</code></pre> Solution:  1. Check job allocation: <pre><code>echo $CUDA_VISIBLE_DEVICES  # Should show GPU ID\nnvidia-smi  # Should list GPUs\n</code></pre></p> <ol> <li>Verify <code>--nv</code> flag in singularity command</li> <li>Check CUDA version compatibility: <pre><code>singularity exec --nv container.sif nvidia-smi\n</code></pre></li> </ol>"},{"location":"troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"troubleshooting/#out-of-memory-oom","title":"Out of memory (OOM)","text":"<p><pre><code>RuntimeError: CUDA out of memory\n</code></pre> Solution: - Reduce batch size - Use gradient accumulation - Request more memory: <code>#SBATCH --mem-per-cpu=8G</code> - Monitor memory usage: <pre><code>import torch\nprint(f\"Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\nprint(f\"Reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n</code></pre></p>"},{"location":"troubleshooting/#permission-denied","title":"Permission denied","text":"<p><pre><code>Permission denied: '/output/results.txt'\n</code></pre> Solution:  - Create output directory first - Check bind mount syntax - Use proper permissions: <pre><code>mkdir -p /cluster/project/rsl/$USER/output\nchmod 755 /cluster/project/rsl/$USER/output\n</code></pre></p>"},{"location":"troubleshooting/#slow-io-performance","title":"Slow I/O performance","text":"<p>Solution: Always use local scratch <pre><code># Good - use $TMPDIR\ncp /cluster/scratch/$USER/data.tar $TMPDIR/\ntar -xf $TMPDIR/data.tar -C $TMPDIR/\n\n# Bad - network I/O\ntar -xf /cluster/scratch/$USER/data.tar -C /cluster/work/$USER/\n</code></pre></p>"},{"location":"troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"troubleshooting/#interactive-debugging","title":"Interactive Debugging","text":"<p>Start an interactive session: <pre><code># Request resources\nsrun --gpus=1 --mem=16G --tmp=50G --time=1:00:00 --pty bash\n\n# Extract container\ntar -xf /cluster/work/rsl/$USER/containers/debug.tar -C $TMPDIR\n\n# Enter container interactively\nsingularity shell --nv $TMPDIR/debug.sif\n\n# Test commands manually\npython3 -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre></p>"},{"location":"troubleshooting/#verbose-output","title":"Verbose Output","text":"<p>Add debugging to job scripts: <pre><code>#!/bin/bash\n#SBATCH --job-name=debug-job\n\n# Enable bash debugging\nset -x\n\n# Print environment\necho \"=== Environment ===\"\nenv | grep -E \"(CUDA|SINGULARITY|SLURM)\" | sort\n\n# Check allocations\necho \"=== Allocations ===\"\necho \"GPUs: $CUDA_VISIBLE_DEVICES\"\necho \"CPUs: $SLURM_CPUS_PER_TASK\"\necho \"Memory: $SLURM_MEM_PER_CPU MB per CPU\"\necho \"Tmp space: $(df -h $TMPDIR | tail -1)\"\n\n# Time each step\necho \"=== Extraction ===\"\ntime tar -xf container.tar -C $TMPDIR\n\necho \"=== Container Info ===\"\nsingularity inspect $TMPDIR/container.sif\n</code></pre></p>"},{"location":"troubleshooting/#common-debug-commands","title":"Common Debug Commands","text":"<pre><code># Check job details\nscontrol show job $SLURM_JOB_ID\n\n# Monitor resource usage\nwatch -n 1 'sstat -j $SLURM_JOB_ID --format=JobID,MaxRSS,MaxDiskRead,MaxDiskWrite'\n\n# Check GPU usage on node\nssh $NODE 'nvidia-smi -l 1'\n\n# View detailed job info after completion\nsacct -j $SLURM_JOB_ID --format=JobID,JobName,Partition,State,ExitCode,Elapsed,MaxRSS,AllocGRES\n\n# Check why job failed\nscontrol show job $SLURM_JOB_ID | grep -E \"(Reason|ExitCode)\"\n</code></pre>"},{"location":"troubleshooting/#performance-optimization","title":"Performance Optimization","text":""},{"location":"troubleshooting/#container-size-optimization","title":"Container Size Optimization","text":"<p>Reduce container size: <pre><code># Multi-stage build\nFROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 AS builder\nRUN apt-get update &amp;&amp; apt-get install -y build-essential\n# Build steps...\n\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\nCOPY --from=builder /app/bin /app/bin\n# Minimal runtime dependencies only\n</code></pre></p>"},{"location":"troubleshooting/#data-loading-optimization","title":"Data Loading Optimization","text":"<pre><code># Use local scratch for datasets\nimport os\nimport shutil\n\n# Copy dataset to local scratch at job start\nif os.environ.get('SLURM_JOB_ID'):\n    local_data = f\"{os.environ['TMPDIR']}/dataset\"\n    if not os.path.exists(local_data):\n        shutil.copytree('/cluster/scratch/user/dataset', local_data)\n    data_path = local_data\nelse:\n    data_path = './dataset'\n\n# Use multiple workers for data loading\ndataloader = DataLoader(dataset, \n                       batch_size=32,\n                       num_workers=8,  # Match CPU count\n                       pin_memory=True,\n                       persistent_workers=True)\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#rsl-specific-support","title":"RSL-Specific Support","text":"<ul> <li>Contact your supervisor</li> <li>Email Manthan Patel for group access issues</li> <li>Check RSL wiki for lab-specific guidelines</li> </ul>"},{"location":"troubleshooting/#cluster-support","title":"Cluster Support","text":"<ul> <li>ETH IT ServiceDesk: help.ethz.ch</li> <li>HPC mailing list: hpc@id.ethz.ch</li> <li>Euler documentation: scicomp.ethz.ch/wiki</li> </ul>"},{"location":"troubleshooting/#community-resources","title":"Community Resources","text":"<ul> <li>GitHub Issues</li> <li>RSL Slack channels</li> <li>ETH HPC user meetings</li> </ul> <p>Back to Home | Container Workflow | Scripts</p>"}]}